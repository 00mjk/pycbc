#!/bin/env  python
"""
This program find the foreground and background coincidence between single
detector gravitational-wave triggers, using  a dynamic  sampling  timeslide
method.
"""
import itertools, copy, numpy, logging, argparse, h5py, os
from pycbc.detector import Detector
from glue import segments

class BinnedLookup(dict):
    """ Lookup  table for a binned column vectors
    """
    def  __init__(self, columns, column_types, bounds):
        """ Create a lookup table  for a binned column vectors.
        
        Parameters
        ----------
        columns : list  of uint32 numpy  arrays
            The column  vectors to instantiate the lookup table
        column_types : 'neighbors', 'exact'
            Type of binning performed  on the column
        bounds : list of tuples (int, int)
            List of tuples of two ints that represent the cyclic boundaries
            of the bins. None if not cyclic. 
        """
        self.columns = columns
        self.column_types = column_types 
        self.bounds = bounds
       
        if  len(columns) != len(column_types) != len(bounds):
            raise ValueError('Number of columns, names, and types must match')
           
        self.columns, self.indices = self._expand_columns() 
        for j, tup in enumerate(itertools.izip(*self.columns)):   
            if tup not in self:
                self[tup] = [self.indices[j]]
            else:
                self[tup].append(self.indices[j])

    def lookup(self, columns):
        """Return two arrays of index and result for matches between
        the binned input columns and this lookup
        
        Parameters
        ----------
        columns : list of binned column vectors to look up in the dictionary
        
        Returns
        index, index : two arrays containing the matched indices
        """
        if len(columns) != len(self.columns):
            raise ValueError('Number of columns, names, and types must match')
        
        d1_indices = []
        d2_indices = []
        for index, tup in enumerate(itertools.izip(*columns)): 
            if tup in self:
                for index2 in self[tup]:
                    d1_indices.append(index)
                    d2_indices.append(index2)
        return (numpy.array(d1_indices, dtype=numpy.uint32), 
               numpy.array(d2_indices, dtype=numpy.uint32) )

    def _cyclic_increment(self, vec, lbound, ubound):
        vec = vec + 1
        if ubound and lbound:
            vec[vec>ubound] = lbound
        return vec
        
    def _cyclic_decrement(self, vec, lbound, ubound):
        vec = vec - 1
        if lbound and lbound:
            vec[vec<lbound] = ubound
        return vec
        
    def _expand_columns(self):
        expanded = copy.deepcopy(self.columns)
        indices = numpy.arange(0, len(expanded[0]), 1).astype(numpy.uint32)
        for i, (c, t) in enumerate(zip(self.columns, self.column_types)):
            if t == 'exact':
                continue
            elif t == 'neighbors':
                for j, (vec, (lb, ub)) in enumerate(zip(expanded, self.bounds)):
                    if i == j:
                        upper = self._cyclic_increment(vec, lb, ub)
                        lower = self._cyclic_decrement(vec, lb, ub)
                        expanded[i] = numpy.concatenate((lower, vec, upper))
                    else:
                        expanded[j] = numpy.resize(vec, len(vec)*3)
                        indices = numpy.resize(indices, len(indices)*3)
        return expanded, indices
        
def bin_vector(vector, window):
    return (vector / window).astype(numpy.uint32)
    
def bin_slide_window(time, slide_step, window):
    bins =  (numpy.remainder(time, slide_step) / window).astype(numpy.uint32)
    return bins
    
def exact_match_binning(time, template, time_step, time_window):
    columns = []
    if time_step is not None:
        columns.append(bin_slide_window(time, time_step, time_window))
    else:
        columns.append(bin_vector(time, time_window)) 
    columns.append(template)
    return columns
    
def test_exact_match_coincidence(index1, index2, time1, time2, slide_step, window):
    coinc_times1 = time1[index1]
    coinc_times2 = time2[index2]
    if slide_step is not None:
        slide_index = numpy.around(((coinc_times2 - coinc_times1) / slide_step)).astype(numpy.int32)
        time_diff = abs(coinc_times2 - coinc_times1 - slide_index * slide_step)
    else:
        slide_index = numpy.zeros(len(index1), dtype=numpy.int32)
        time_diff = abs(coinc_times2 - coinc_times1)
    passes = (time_diff <= window)
    return index1[passes], index2[passes], slide_index[passes]
    
def load_triggers(trigger_files, prefix):
    triggers = {}
    trig_segments = {}
    num_trigs = 0
    
    for i, filename in enumerate(trigger_files):
        logging.info('Loading file: %s' % filename)
        f = h5py.File(filename, "r")
        
        try:
            snr = f[prefix + '/stat'][:]
            end_time = f[prefix + '/end_time'][:]
            th = f[prefix + '/template_id'][:]
            ifo = f.attrs['ifo']
        except ValueError:
            continue

        # Compile a complete segment list for the foreground time
        if ifo not in trig_segments:
            trig_segments[ifo] = segments.segmentlist()  
          
        seg = segments.segment(f.attrs['start_time'], f.attrs['end_time'])
        trig_segments[ifo] += [seg]

        if len(snr) > 0:
            if ifo not in triggers:
                triggers[ifo] = ([], [], [])

            sl, el, tl = triggers[ifo]
            sl += [snr]
            el += [end_time]
            tl += [th]

            num_trigs += len(snr)
        logging.info("%s/%s : Total Trigs=%s: %s-%s" % (i+1, len(trigger_files), num_trigs, snr.min(), snr.max()))
    
    for key in triggers.keys():
        sl, el, tl = triggers[key]
        triggers[key] = (numpy.concatenate(sl), 
                         numpy.concatenate(el),
                         numpy.concatenate(tl))
    return triggers, trig_segments
    
def decimate_triggers(s1, s2, slide, num_keep, bins, factor=2):
    """ Remove coinc triggers that are unlikely to be significant in
    determining in affecting the error bars of a FAP calculation
    
    Parameters
    ----------
    s1 : numpy array
        The values of the first detector's ranking statistic
    s2 : numpy array
        The values of the second detector's ranking statistic
    slide : numpy array
        The array of timeslide id values that identify the unique timeslide
        this trigger was coincident for. 
    num_keep : int 
        The number of triggers that will be kept at the highest ranking
    statistic value without any decimation.
    bins : int
        The number of network statistic bins to perform separate decimation
    in. The boundaries are divided evenly between the statistic value range.
    factor : int
        The factor to decimate each bin by. This factor of slides will be
    removed.
    
    """
    netstatsq = s1 ** 2.0 + s2 ** 2.0
    stat_sort = netstatsq.argsort()
    netstatsq = netstatsq[stat_sort]
    slide = slide[stat_sort]
    
    # keep the number of loud triggers
    keep = numpy.arange(len(s1) - num_keep, len(s1), 1)
    factors = numpy.zeros(len(keep), dtype=numpy.uint32) + 1
    netstatsq = netstatsq[0:len(s1) - num_keep]
    slide = slide[0:len(s1) - num_keep]

    min_slide = slide.min()
    max_slide = slide.max()
 
    edges = [netstatsq[0] + inc*(netstatsq[-1]-netstatsq[0])/bins for inc in range(bins)]
    bin_edges = numpy.append(numpy.searchsorted(netstatsq, edges), len(netstatsq))
    for i in range(bins):
        lbin = bin_edges[-i-2]
        rbin = bin_edges[-i-1]
    
        keep_upper = numpy.arange(0, max_slide + factor, factor)
        keep_lower = numpy.arange(-factor, min_slide - factor, -factor)
        keep_slides = numpy.unique(numpy.append(keep_upper, keep_lower))

        
        bin_int = numpy.in1d(slide[lbin:rbin], keep_slides)
        bin_keep = numpy.where(bin_int)[0] + lbin
        
        factors = numpy.append(factors, numpy.zeros(len(bin_keep))+factor)
        keep = numpy.append(keep, bin_keep)
        factor = factor * 2   

    return stat_sort[keep], factors.astype(numpy.uint32)

def veto_indices(times, ifo, veto_files):
    """ Return the list of indices that should be vetoed by the segments in the
    lsit of veto_files.
    """
    time_sorting = numpy.argsort(times)
    times = times[time_sorting]
    indices = numpy.array([], dtype=numpy.uint32)

    from glue.ligolw import ligolw, table, lsctables, utils as ligolw_utils
    class LIGOLWContentHandler(ligolw.LIGOLWContentHandler):
        pass
    lsctables.use_in(LIGOLWContentHandler)
   
    for veto_file in veto_files:
        indoc = ligolw_utils.load_filename(veto_file, False, contenthandler=LIGOLWContentHandler)
        segment_table  = table.get_table(indoc, lsctables.SegmentTable.tableName)
        
        seg_def_table = table.get_table(indoc, lsctables.SegmentDefTable.tableName)
        def_ifos = seg_def_table.getColumnByName('ifos')
        def_ids = seg_def_table.getColumnByName('segment_def_id')
        ifo_map =  {}
        for def_ifo, def_id in zip(def_ifos, def_ids):
            ifo_map[def_id] = def_ifo
        
        start = numpy.array(segment_table.getColumnByName('start_time')) + numpy.array(segment_table.getColumnByName('start_time_ns')) * 1e-9
        end = numpy.array(segment_table.getColumnByName('end_time')) + numpy.array(segment_table.getColumnByName('end_time_ns')) * 1e-9
        ifos = [ifo_map[v] for v in segment_table.getColumnByName('segment_def_id')]
        
        veto_segs = segments.segmentlist()
        for s, e, ifo_row in zip(start, end, ifos):
            if ifo != ifo_row:
                continue
                
            veto_segs += [segments.segment(s, e)]
        
        left = numpy.searchsorted(times, start, side='left')
        right = numpy.searchsorted(times, end, side='right')   
        for li, ri, ifo_row in zip(left, right, ifos):
            if ifo != ifo_row:
                continue
                
            seg_indices = numpy.arange(li, ri, 1).astype(numpy.uint32)
            indices=numpy.union1d(seg_indices, indices)  
            
    return time_sorting[indices], veto_segs

def cluster_coincs(stat1, stat2, time1, time2, timeslide_id, template_id):
    """Cluster coincident events for each timeslide separately, across 
    templates, based on the loudest network statistic (stat1**2 + stat2**2). 
    Return the set of indices corresponding to the surviving coincidences.
    """
    nstatsq = stat1**2.0 + stat2**2.0
    

if __name__ == '__main__': 
    parser = argparse.ArgumentParser()
    # General required options
    parser.add_argument('--trigger-files', nargs='+')
    parser.add_argument('--veto-files', nargs='+')
    # Could be different ways to communicate this information, think about it
    parser.add_argument('--timeslide-interval', type=float, default=None)
    parser.add_argument('--hdf-prefix')
    parser.add_argument('--verbose', '-v', action='count')
    parser.add_argument('--output-file')
    parser.add_argument('--decimation-factor', type=int)
    parser.add_argument('--decimation-bins', type=int)
    parser.add_argument('--decimation-keep', type=int)
    args = parser.parse_args()

    if args.verbose == 1:
        log_level = logging.INFO
    elif args.verbose == 2:
        log_level = logging.DEBUG
    else:
        log_level = logging.WARN
    logging.basicConfig(format='%(asctime)s : %(message)s', level=log_level)       

    triggers, foreground_segments = load_triggers(args.trigger_files, args.hdf_prefix)

    ifo1, ifo2 = triggers.keys()
    s1, t1, r1 = triggers[ifo1]
    s2, t2, r2 = triggers[ifo2]
    fore_time1 = abs(foreground_segments[ifo1])
    fore_time2 = abs(foreground_segments[ifo2])
    
    logging.info('%s time: %s  %s time: %s' %  (ifo1, fore_time1, ifo2, fore_time2))
    
    logging.info('%s loudest: %s  %s loudest: %s' %  (ifo1, s1.max(), ifo2, s2.max()))
    
    logging.info('%s loudest: %s  %s loudest: %s' %  (ifo1, s1.max(), ifo2, s2.max()))
    
    if  args.veto_files:
        logging.info('applying vetoes')
        veto1, vsegs1 = veto_indices(t1, ifo1, args.veto_files)
        s1, t1, r1 = numpy.delete(s1, veto1), numpy.delete(t1, veto1), numpy.delete(r1, veto1)
        fore_time1 -= abs(foreg_time1 and vsegs1)

        veto2, vsegs2 = veto_indices(t1, ifo2, args.veto_files)     
        s2, t2, r2 = numpy.delete(s2, veto2), numpy.delete(t2, veto2), numpy.delete(r2, veto2)
        fore_time2 -= abs(fore_time2 and vsegs2)
        logging.info('%s time: %s  %s time: %s' %  (ifo1, fore_time1, ifo2, fore_time2))   
        
    det1 = Detector(ifo1)
    det2 = Detector(ifo2)
    logging.info('%s : %s %s: %s' % (ifo1, len(s1), ifo2, len(s2)))
    time_window = det1.light_travel_time_to_detector(det2)
    
    logging.info('%s loudest: %s  %s loudest: %s' %  (ifo1, s1.max(), ifo2, s2.max()))
    
    logging.info('%s loudest: %s  %s loudest: %s' %  (ifo1, s1.max(), ifo2, s2.max()))
    
    slide = args.timeslide_interval  
    logging.info('making bins') 
    c1s = exact_match_binning(t1, r1, slide, time_window)
    c2s = exact_match_binning(t2, r2, slide, time_window)
    
    logging.info('making lookup table')     
    if args.timeslide_interval is not None: 
        logging.info('looking for foreground and background events')
        course = BinnedLookup(c2s, ['neighbors', 'exact'],
                                   [(0, int(slide/time_window)), (None, None)])
        num_slides = int(max(fore_time1, fore_time2) / args.timeslide_interval)
    else:
        logging.info('only looking for foreground events')
        course = BinnedLookup(c2s, ['neighbors', 'exact'], 
                                   [(None, None), (None, None)]) 
        num_slides = 0
        
    logging.info('Number of timeslides: %s' % num_slides)

    logging.info('finding course coincidences')                  
    i1, i2 = course.lookup(c1s)
    
    logging.info('finding coincidences')   
    c1, c2, slide_index = test_exact_match_coincidence(i1, i2, t1, t2, slide, time_window)
    s1, s2, t1, t2, r1, r2 = s1[c1], s2[c2], t1[c1], t2[c2], r1[c1], r2[c2]   
    logging.info('%s total triggers' % len(s1))
    logging.info('%s foreground triggers' % (slide_index == 0).sum())
    
    if args.decimation_keep < len(s1) and args.timeslide_interval is not None: 
        logging.info('decimating triggers')
        d1, decimation_factor = decimate_triggers(s1, s2, slide_index, 
                               args.decimation_keep, 
                               args.decimation_bins, 
                               args.decimation_factor)
        s1, s2, t1, t2, r1, r2 = s1[d1], s2[d1], t1[d1], t2[d1], r1[d1], r2[d1]
        slide_index = slide_index[d1]
        logging.info('%s total triggers' % len(s1))
        logging.info('%s foreground triggers' % (slide_index == 0).sum())
    else:
        decimation_factor 
    
    logging.info('saving coincident triggers')
    f = h5py.File(args.output_file, "w")
    
    if len(c1) > 0:
        f.create_dataset('coinc/stat1', data=s1)
        f.create_dataset('coinc/decimation_factor', data=decimation_factor)
        f.create_dataset('coinc/stat2', data=s2)
        f.create_dataset('coinc/time1', data=t1)
        f.create_dataset('coinc/time2', data=t2)
        f.create_dataset('coinc/timeslide_id', data=slide_index)
        f.create_dataset('coinc/template_id', data=r1)
        f.attrs['timeslide_interval'] = numpy.float64(args.timeslide_interval)
        f.attrs['detector_1'] = det1.name
        f.attrs['detector_2'] = det2.name
        f.attrs['foreground_time1'] = fore_time1
        f.attrs['foreground_time1'] = fore_time1
        f.attrs['num_slides'] = num_slides
    else:
        logging.info('huh, why are there no triggers?')

    logging.info('Done')
