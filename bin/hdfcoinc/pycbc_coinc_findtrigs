#!/bin/env  python
"""
This program find the foreground and background coincidence between single
detector gravitational-wave triggers, using  a dynamic  sampling  timeslide
method.
"""
import itertools, copy, logging, argparse, h5py, os
from pycbc.future import numpy
from pycbc.detector import Detector
import pycbc.events
from glue import segments
from pycbc.events import indices_within_segments as veto_indices

def get_statistic_class(option):
    if option == 'newsnr':
        return NewSNRStatistic()
    elif option == 'newsnr_volume':
        return NewSNRVolStatistic()
    elif option == 'newsnr_phasehl':
        return NewSNRPhaseStatistic()
    elif option == 'newsnr_phase_basvol_hl':
        return NewSNRVolStatistic()

class NewSNRStatistic(object):
    """ ABS Class to handle information about detection statistics
    """
    def single_read(self, f, l, r):
        """ Read in the single detector information and make a single detector
        statistic. Results can either be a single number or a record array.
        """
        x2_dof = 2 * f.attrs['chisq_dof'] - 2
        snr = pycbc.events.newsnr(f['snr'][l:r], f['chisq'][l:r] / x2_dof)
        return numpy.array(snr, ndmin=1)

    def coinc_statistic(self, s1, s2):
        """ Calculate the coincident statistic.
        """
        return (s1**2.0 + s2**2.0) ** 0.5
        
class NewSNRPhaseStatistic(object):
    """ ABS Class to handle information about detection statistics
    """
    def phase_log_pdf(self, p1, p2):
        pdiff = abs(p2 - p1) / numpy.pi - 1
        sigsq = (1.0 / numpy.pi) ** 2.0      
        
        # This ignores many factors that are simply constants in the loglikelihood
        logsig = -1.0 * (pdiff)**2.0 / 2.0 / sigsq
        return logsig
    
    def single_read(self, f, l, r):
        """ Read in the single detector information and make a single detector
        statistic. Results can either be a single number or a record array.
        """
        x2_dof = 2 * f.attrs['chisq_dof'] - 2
        snr = pycbc.events.newsnr(f['snr'][l:r], f['chisq'][l:r] / x2_dof)
        snr = numpy.array(snr, ndmin=1)
        phase = numpy.array(f['coa_phase'][l:r], ndmin=1)
        stat1 = numpy.zeros(len(snr), dtype=[('snr', numpy.float32), ('phase', numpy.float32)])
        stat1['phase'] = phase
        stat1['snr'] = snr
        return stat1
    
    def coinc_statistic(self, s1, s2):
        snr1, snr2 = s1['snr'], s2['snr']
        p1, p2 = s1['phase'], s2['phase']
        stat = (snr1**2.0 + snr2**2.0) / 2.0 + self.phase_log_pdf(p1, p2)
        return stat**0.5

class NewSNRVolStatistic(NewSNRPhaseStatistic):
    """ ABS Class to handle information about detection statistics
    """    
    def single_read(self, f, l, r):
        """ Read in the single detector information and make a single detector
        statistic. Results can either be a single number or a record array.
        """
        from pycbc.waveform.spa_tmplt import spa_amplitude_factor
        x2_dof = 2 * f.attrs['chisq_dof'] - 2
        snr = pycbc.events.newsnr(f['snr'][l:r], f['chisq'][l:r] / x2_dof)
        snr = numpy.array(snr, ndmin=1)
        phase = numpy.array(f['coa_phase'][l:r], ndmin=1)

        amp = spa_amplitude_factor(mass1=f['mass1'][l:r], mass2=f['mass2'][l:r])
        ref_amp = spa_amplitude_factor(mass1=1.4, mass2=1.4)
        
        dist_fact = numpy.sqrt(f['sigmasq'][l:r]) / 1000.0 * ref_amp / amp

        stat1 = numpy.zeros(len(snr), dtype=[('snr', numpy.float32), 
                                             ('phase', numpy.float32),
                                             ('vol', numpy.float32)])
        stat1['phase'] = phase
        stat1['snr'] = snr
        stat1['vol'] = dist_fact ** 3.0
        return stat1

    def coinc_statistic(self, s1, s2):
        snr1, snr2 = s1['snr'], s2['snr']
        p1, p2 = s1['phase'], s2['phase']
        v1, v2 = s1['vol'], s2['vol']
        stat = (snr1**2.0 + snr2**2.0) / 2.0 + self.phase_log_pdf(p1, p2) + numpy.log(numpy.minimum(v1, v2))
        return stat**0.5

class ArrayList(list):
    def get(self):
        try:
            return numpy.concatenate(self)
        except:
	    return numpy.array([], ndmin=1, dtype=numpy.uint32)

class BinnedLookup(dict):
    """ Lookup  table for a binned column vectors
    """
    def  __init__(self, columns, column_types, bounds):
        """ Create a lookup table  for a binned column vectors.
        
        Parameters
        ----------
        columns : list  of uint32 numpy  arrays
            The column  vectors to instantiate the lookup table
        column_types : 'neighbors', 'exact'
            Type of binning performed  on the column
        bounds : list of tuples (int, int)
            List of tuples of two ints that represent the cyclic boundaries
            of the bins. None if not cyclic. 
        """
        self.columns = columns
        self.column_types = column_types 
        self.bounds = bounds
       
        if  len(columns) != len(column_types) != len(bounds):
            raise ValueError('Number of columns, names, and types must match')
           
        self.columns, self.indices = self._expand_columns() 
        for j, tup in enumerate(itertools.izip(*self.columns)):   
            if tup not in self:
                self[tup] = [self.indices[j]]
            else:
                self[tup].append(self.indices[j])

    def lookup(self, columns):
        """Return two arrays of index and result for matches between
        the binned input columns and this lookup
        
        Parameters
        ----------
        columns : list of binned column vectors to look up in the dictionary
        
        Returns
        index, index : two arrays containing the matched indices
        """
        if len(columns) != len(self.columns):
            raise ValueError('Number of columns, names, and types must match')
        
        d1_indices = ArrayList()
        d2_indices = ArrayList()

        for index, tup in enumerate(itertools.izip(*columns)): 
            if tup in self:
                val = self[tup]
                indices = numpy.empty(len(val), dtype=numpy.uint32)
                indices.fill(index)
                indices2 = numpy.array(val, dtype=numpy.uint32)
    
                d1_indices.append(indices)
                d2_indices.append(indices2)
        logging.info('lookup done')        

        return d1_indices.get(), d2_indices.get()

    def _cyclic_increment(self, vec, lbound, ubound):
        vec = vec + 1
        if ubound and lbound:
            vec[vec>ubound] = lbound
        return vec
        
    def _cyclic_decrement(self, vec, lbound, ubound):
        vec = vec - 1
        if lbound and lbound:
            vec[vec<lbound] = ubound
        return vec
        
    def _expand_columns(self):
        expanded = copy.deepcopy(self.columns)
        indices = numpy.arange(0, len(expanded[0]), 1).astype(numpy.uint32)
        for i, (c, t) in enumerate(zip(self.columns, self.column_types)):
            if t == 'exact':
                continue
            elif t == 'neighbors':
                for j, (vec, (lb, ub)) in enumerate(zip(expanded, self.bounds)):
                    if i == j:
                        upper = self._cyclic_increment(vec, lb, ub)
                        lower = self._cyclic_decrement(vec, lb, ub)
                        expanded[i] = numpy.concatenate((lower, vec, upper))
                    else:
                        expanded[j] = numpy.resize(vec, len(vec)*3)
                        indices = numpy.resize(indices, len(indices)*3)
        return expanded, indices
        
def bin_vector(vector, window):
    return (vector / window).astype(numpy.uint32)
    
def bin_slide_window(time, slide_step, window):
    bins =  (numpy.remainder(time, slide_step) / window).astype(numpy.uint32)
    return bins
    
def exact_match_binning(time, template, time_step, time_window):
    columns = []
    if time_step is not None:
        columns.append(bin_slide_window(time, time_step, time_window))
    else:
        columns.append(bin_vector(time, time_window)) 
    columns.append(template)
    return columns
    
def test_exact_match_coincidence(index1, index2, time1, time2, slide_step, window):
    coinc_times1 = time1[index1]
    coinc_times2 = time2[index2]
    if slide_step is not None:
        slide_index = numpy.around(((coinc_times2 - coinc_times1) / slide_step)).astype(numpy.int32)
        time_diff = abs(coinc_times2 - coinc_times1 - slide_index * slide_step)
    else:
        slide_index = numpy.zeros(len(index1), dtype=numpy.int32)
        time_diff = abs(coinc_times2 - coinc_times1)
    passes = (time_diff <= window)
    return index1[passes], index2[passes], slide_index[passes]
    
def load_triggers(trigger_files, group_id, stat_class):
    triggers = {}
    group_id = int(group_id)
    trig_segments = {}
    num_trigs = 0
    
    for i, filename in enumerate(trigger_files):
        logging.info('Loading file: %s' % filename)
        f = h5py.File(filename, "r")
        ifo = str(f.attrs['ifo'])

        # Test if file is empty
        try:
            f['snr']
        except KeyError:
            continue

        # Compile a complete segment list for the foreground time
        if ifo not in trig_segments:
            trig_segments[ifo] = segments.segmentlist()  
          
        for s, e in zip(f['search/start_time'][:], f['search/end_time'][:]):
            trig_segments[ifo] += [segments.segment(s,e)]
        
        l = f['template_group/left'][group_id]
        r = f['template_group/right'][group_id]

        if (r-l) > 0:
            end_time = f['end_time'][l:r]
            th = f['template_id'][l:r]
            snr = stat_class.single_read(f, l, r)
            
            if ifo not in triggers:
                triggers[ifo] = ([], [], [])

            sl, el, tl = triggers[ifo]
            sl += [snr]
            el += [end_time]
            tl += [th]

            num_trigs += len(snr)
            logging.info("%s/%s : Total Trigs=%s:" % (i+1, len(trigger_files), num_trigs))
        f.close()
    for key in triggers.keys():
        sl, el, tl = triggers[key]
        triggers[key] = (numpy.concatenate(sl), 
                         numpy.concatenate(el),
                         numpy.concatenate(tl))
    return triggers, trig_segments
    
def decimate_triggers(stat, slide, num_keep, bins, factor=2):
    """ Remove coinc triggers that are unlikely to be significant in
    determining in affecting the error bars of a FAP calculation
    
    Parameters
    ----------
    stat : numpy array
        The values of the ranking statistic
    slide : numpy array
        The array of timeslide id values that identify the unique timeslide
        this trigger was coincident for. 
    num_keep : int 
        The number of triggers that will be kept at the highest ranking
    statistic value without any decimation.
    bins : int
        The number of network statistic bins to perform separate decimation
    in. The boundaries are divided evenly between the statistic value range.
    factor : int
        The factor to decimate each bin by. This factor of slides will be
    removed.
    
    """
    stat_sort = stat.argsort()
    stat = stat[stat_sort]
    slide = slide[stat_sort]
    
    # keep the number of loud triggers
    keep = numpy.arange(len(stat) - num_keep, len(stat), 1)
    factors = numpy.zeros(len(keep), dtype=numpy.uint32) + 1
    stat = stat[0:len(stat) - num_keep]
    slide = slide[0:len(stat) - num_keep]

    min_slide = slide.min()
    max_slide = slide.max()
 
    if bins > 0:
        edges = [stat[0] + inc*(stat[-1]-stat[0])/bins for inc in range(bins)]
        bin_edges = numpy.append(numpy.searchsorted(stat, edges), len(stat))
        for i in range(bins):
            lbin = bin_edges[-i-2]
            rbin = bin_edges[-i-1]
        
            keep_upper = numpy.arange(0, max_slide + factor, factor)
            keep_lower = numpy.arange(-factor, min_slide - factor, -factor)
            keep_slides = numpy.unique(numpy.append(keep_upper, keep_lower))
            
            bin_int = numpy.in1d(slide[lbin:rbin], keep_slides)
            bin_keep = numpy.where(bin_int)[0] + lbin
            
            factors = numpy.append(factors, numpy.zeros(len(bin_keep))+factor)
            keep = numpy.append(keep, bin_keep)
            factor = factor * 2   

    return stat_sort[keep], factors.astype(numpy.uint32)

if __name__ == '__main__': 
    parser = argparse.ArgumentParser()
    # General required options
    parser.add_argument('--trigger-files', nargs='+', help='List of hdf format single detector inspiral trigger files.')
    parser.add_argument('--veto-files', nargs='+', help='List of veto files to apply to the single detector triggers.')
    # Could be different ways to communicate this information, think about it
    parser.add_argument('--timeslide-interval', type=float, default=None, help='Interval in seconds between timeslide offsets')
    parser.add_argument('--template-group', nargs='+', help='Template group index to analyze')
    parser.add_argument('--verbose', '-v', action='count')
    parser.add_argument('--output-file')
    parser.add_argument('--decimation-factor', type=int, help='Factor to decimate triggers by for each bin')
    parser.add_argument('--decimation-bins', type=int, help='Number of decimation intervals')
    parser.add_argument('--decimation-keep', type=int, help='Number of the most significant backgroud triggers to keep without decimation')
    parser.add_argument('--coinc-threshold', type=float, default=0, help='Fixed time to add to the time-of-flight coincidence window size in seconds')
    parser.add_argument('--coinc-statistic', default='newsnr', help='The coinc statistic to calculate. The default is newsnr')
    args = parser.parse_args()
    
    stat_class = get_statistic_class(args.coinc_statistic)

    if args.verbose == 1:
        log_level = logging.INFO
    elif args.verbose == 2:
        log_level = logging.DEBUG
    else:
        log_level = logging.WARN
    logging.basicConfig(format='%(asctime)s : %(message)s', level=log_level)       

    stat_all = []
    t1_all, t2_all = [], []
    dec_all = []
    slide_all = []
    template_all = []
    i1_all, i2_all = [], []
    
    for template_group in args.template_group:
        logging.info('Loading triggers from template group %s' % template_group)
        triggers, foreground_segments = load_triggers(args.trigger_files, template_group, stat_class)

        ifo1, ifo2 = triggers.keys()
        s1, t1, r1 = triggers[ifo1]
        s2, t2, r2 = triggers[ifo2]
        fore_time1 = abs(foreground_segments[ifo1])
        fore_time2 = abs(foreground_segments[ifo2])
        coinc_segs = (foreground_segments[ifo1] & foreground_segments[ifo2]).coalesce()
        coinc_time = abs(coinc_segs)
        i1 = numpy.arange(0, len(s1))
        i2 = numpy.arange(0, len(s2))
        
        start = []
        end = []
        for seg in coinc_segs:
            start.append(seg[0])
            end.append(seg[1])
        start = numpy.array(start)
        end = numpy.array(end)    
        
        logging.info('%s time: %s  %s time: %s' %  (ifo1, fore_time1, ifo2, fore_time2))           
        if  args.veto_files:
            logging.info('applying vetoes')
            veto1, vsegs1 = veto_indices(t1, ifo1, args.veto_files)
            s1, t1, r1, i1 = numpy.delete(s1, veto1), numpy.delete(t1, veto1), numpy.delete(r1, veto1), numpy.delete(i1, veto1)
            fore_time1 -= abs(foreground_segments[ifo1] & vsegs1)

            veto2, vsegs2 = veto_indices(t2, ifo2, args.veto_files)     
            s2, t2, r2, i2 = numpy.delete(s2, veto2), numpy.delete(t2, veto2), numpy.delete(r2, veto2), numpy.delete(i2, veto2)
            fore_time2 -= abs(foreground_segments[ifo2] & vsegs2)
            logging.info('%s time: %s  %s time: %s' %  (ifo1, fore_time1, ifo2, fore_time2))   
            
        det1 = Detector(ifo1)
        det2 = Detector(ifo2)
        logging.info('%s num: %s   %s num: %s' % (ifo1, len(s1), ifo2, len(s2)))
        time_window = det1.light_travel_time_to_detector(det2) + args.coinc_threshold
        
        slide = args.timeslide_interval  
        logging.info('making bins') 
        c1s = exact_match_binning(t1, r1, slide, time_window)
        c2s = exact_match_binning(t2, r2, slide, time_window)
        
        logging.info('making lookup table')     
        if args.timeslide_interval is not None: 
            logging.info('looking for foreground and background events')
            course = BinnedLookup(c2s, ['neighbors', 'exact'],
                                       [(0, int(slide/time_window)), (None, None)])
            num_slides = int(max(fore_time1, fore_time2) / args.timeslide_interval)
        else:
            logging.info('only looking for foreground events')
            course = BinnedLookup(c2s, ['neighbors', 'exact'], 
                                       [(None, None), (None, None)]) 
            num_slides = 0
            
        logging.info('Number of timeslides: %s' % num_slides)
        logging.info('finding course coincidences')                  
        c1, c2 = course.lookup(c1s)

        logging.info('finding coincidences')   
        c1, c2, slide_index = test_exact_match_coincidence(c1, c2, t1, t2, slide, time_window)

        logging.info('%s total triggers' % len(c1))
        logging.info('%s foreground triggers' % (slide_index == 0).sum())
        
        logging.info('calculating the coinc statistic')
        stat = stat_class.coinc_statistic(s1[c1], s2[c2])
        
        if args.decimation_keep < len(stat) and args.timeslide_interval is not None: 
            logging.info('decimating triggers')
            d1, decimation_factor = decimate_triggers(stat, slide_index, 
                                   args.decimation_keep, 
                                   args.decimation_bins, 
                                   args.decimation_factor)                                 
            e1 = c1[d1]
            e2 = c2[d1]
            stat, t1, t2, r1, i1, i2 = stat[d1], t1[e1], t2[e2], r1[e1], i1[e1], i2[e2]
            slide_index = slide_index[d1]
            logging.info('%s total triggers' % len(stat))
            logging.info('%s foreground triggers' % (slide_index == 0).sum())
        else:
            t1, t2, r1, i1, i2 = t1[c1], t2[c2], r1[c1], i1[c1], i2[c2]
            decimation_factor = numpy.zeros(len(stat), dtype=numpy.uint32) + 1
        
        # Accumulate the coinc triggers from this groups of templates
        stat_all += [stat]
        t1_all += [t1]
        t2_all += [t2]
        dec_all += [decimation_factor]
        slide_all += [slide_index]
        template_all += [r1]
        i1_all, i2_all = i1_all + [i1], i2_all + [i2]
        
    logging.info('saving coincident triggers')
    f = h5py.File(args.output_file, "w")
    
    if len(stat_all) > 0:
        f.create_dataset('coinc/stat', data=numpy.concatenate(stat_all))
        f.create_dataset('coinc/decimation_factor', data=numpy.concatenate(dec_all))
        f.create_dataset('coinc/time1', data=numpy.concatenate(t1_all))
        f.create_dataset('coinc/time2', data=numpy.concatenate(t2_all))
        f.create_dataset('coinc/trigger_id1', data=numpy.concatenate(i1_all))
        f.create_dataset('coinc/trigger_id2', data=numpy.concatenate(i2_all))
        f.create_dataset('coinc/timeslide_id', data=numpy.concatenate(slide_all))
        f.create_dataset('coinc/template_id', data=numpy.concatenate(template_all))
        f['coinc/analyzed_start'] = start
        f['coinc/analyzed_end'] = end
        f.attrs['timeslide_interval'] = numpy.float64(args.timeslide_interval)
        f.attrs['detector_1'] = det1.name
        f.attrs['detector_2'] = det2.name
        f.attrs['foreground_time1'] = fore_time1
        f.attrs['foreground_time2'] = fore_time2
        f.attrs['coinc_time'] = coinc_time
        f.attrs['num_slides'] = num_slides
    else:
        logging.info('huh, why are there no triggers?')

    logging.info('Done')
