#!/usr/bin/env python
import h5py, argparse, logging, numpy, numpy.random
from pycbc import events, detector
from pycbc.events import veto, coinc
       
parser = argparse.ArgumentParser()
parser.add_argument("--verbose", action="count")
parser.add_argument("--veto-files", 
                    help="Optional veto file. Injections within these times are ignored")
parser.add_argument("--trigger-files", nargs=2,
                    help="File containing the single-detector triggers")
parser.add_argument("--template-bank",
                    help="Template bank file in HDF format")
parser.add_argument("--ranking-statistic", help="The ranking statistic to use", default='newsnr')
parser.add_argument("--coinc-threshold", type=float,
                    help="Seconds to add to time-of-flight coincidence window")
parser.add_argument("--timeslide-interval", type=float, default=0,
                    help="Inverval between timeslides in seconds.")
parser.add_argument("--decimation-factor", type=int, default=1000,
                    help="The factor to reduce the background trigger rate.")
parser.add_argument("--loudest-keep", type=int, default=200,
                    help="Number of the loudest triggers to keep from each template.")
parser.add_argument("--template-fraction-range", help="Optional, format string to"
                    "analyze part of template bank. Format is PART/NUM_PARTS",
                    default="0/1")
parser.add_argument("--output-file", help="File to store the coincident triggers")
args = parser.parse_args()    
                   
if args.verbose:
    logging.basicConfig(format='%(asctime)s : %(message)s', level=logging.DEBUG)

def get_statistic(option):
    if option == 'newsnr':
        return NewSNRStatistic()
    else:
        raise ValueError('%s is not an available detection statistic' % option)

def parse_template_range(num_templates, rangestr):
    part = int(rangestr.split('/')[0])
    pieces = int(rangestr.split('/')[1])
    tmin =  int(num_templates / float(pieces) * part)
    tmax =  int(num_templates / float(pieces) * (part+1))
    return tmin, tmax    

class NewSNRStatistic(object):
    def single(self, trigs):
        """ Read in the single detector information and make a single detector
        statistic. Results can either be a single number or a record array.
        """
        dof = 2 * trigs['chisq_dof'] - 2
        newsnr = events.newsnr(trigs['snr'], trigs['chisq'] / dof)
        return numpy.array(newsnr, ndmin=1, dtype=numpy.float32)

    def coinc(self, s1, s2):
        """ Calculate the coincident statistic.
        """
        return (s1**2.0 + s2**2.0) ** 0.5

class ReadByTemplate(object):
    def __init__(self, filename, tmin, tmax, vetofile=None):
        self.filename = filename
        self.tmin = tmin
        self.tmax = tmax
        self.file = h5py.File(filename, 'r')
        self.template_num = None
        self.__dict__.update(self.file.attrs)
        self.data = {}
        
        from glue.segments import segmentlist, segment
        s, e = self.file['search/start_time'][:], self.file['search/end_time'][:]
        self.segs = veto.start_end_to_segments(s, e).coalesce()
        
        template_id, self.template_sort, self.sort = self.make_template_index(tmin, tmax)
        
        if vetofile is not None:
            time, _ = self.chunk_read('end_time', tmin, tmax)
            idx, veto_segs = veto.indices_within_segments(time[self.sort], self.ifo, [vetofile])
            self.template_sort = numpy.delete(self.template_sort, idx)
            template_id = numpy.delete(template_id, idx)
            self.segs = (self.segs - veto_segs).coalesce()
            self.sort = numpy.delete(self.sort, idx)

        self.template_indices = self.group_starts(template_id)
        self.template_map = {}
        for i in range(len(self.template_indices) - 1):
            num = template_id[self.template_indices[i]]
            self.template_map[num] = slice(self.template_indices[i], self.template_indices[i+1])
        self.num_templates = len(self.template_indices) - 1
        
    def chunk_read(self, col, tmin, tmax):
        logging.info('Chunk reading %s start' % col)
        chunks = 100
        step = len(self.file['template_id']) / float(chunks)
        data, locs = [], []
        valid = numpy.arange(tmin, tmax)
        for i in range(chunks):
            s, e = int(i * step), int((i + 1) * step)
            tid = self.file['template_id'][s:e]
            if col != 'template_id':
                dat = self.file[col][s:e]
            else:
                dat = tid
            loc = numpy.where(numpy.in1d(tid, valid))[0]
            data += [dat[loc]]
            locs += [loc + s]
        data = numpy.concatenate(data)
        locs = numpy.concatenate(locs)
        logging.info('Chunk reading %s end' % col)
        return data, locs
   
    def make_template_index(self, tmin, tmax):
        tid, locs = self.chunk_read('template_id', tmin, tmax)
        sort = tid.argsort()
        return tid[sort], locs[sort], sort
        
    def group_starts(self, v):
        return numpy.concatenate([[0], (numpy.where(v[:-1] != v[1:])[0]) + 1, [len(v)]])  
        
    def set_template(self, num):
        self.template_num = num
        self.slice = self.template_map[num]
        self.idx = self.template_sort[self.slice] 
        return self.idx
        
    def __getitem__(self, col):
        if self.template_num == None:
            raise ValueError('You must call set_template to first pick the '
                             'template to read data from')
        if col not in self.data:
            data, _ = self.chunk_read(col, tmin, tmax)
            self.data[col] = data[self.sort]
        return self.data[col][self.slice]

logging.info('Starting...')

num_templates = len(h5py.File(args.template_bank)['template_hash'])
tmin, tmax = parse_template_range(num_templates, args.template_fraction_range)
logging.info('Analyzing template %s - %s' % (tmin, tmax-1))

logging.info('Opening first trigger file: %s' % args.trigger_files[0]) 
trigs0= ReadByTemplate(args.trigger_files[0], tmin, tmax, args.veto_files)
logging.info('Opening second trigger file: %s' % args.trigger_files[1]) 
trigs1 = ReadByTemplate(args.trigger_files[1], tmin, tmax, args.veto_files)
coinc_segs = (trigs0.segs & trigs1.segs).coalesce()

rank_method = get_statistic(args.ranking_statistic)
det0, det1 = detector.Detector(trigs0.ifo), detector.Detector(trigs1.ifo)
time_window = det0.light_travel_time_to_detector(det1) + args.coinc_threshold
logging.info('The coincidence window is %3.1f ms' % (time_window * 1000))

data = {'stat':[], 'decimation_factor':[], 'time1':[], 'time2':[], 
        'trigger_id1':[], 'trigger_id2':[], 'timeslide_id':[], 'template_id':[]}

for tnum in trigs0.template_map:
    tid0 = trigs0.set_template(tnum)
    tid1 = trigs1.set_template(tnum)
    t0 = trigs0['end_time']
    t1 = trigs1['end_time']
    logging.info('Trigs for template %s, %s:%s %s:%s' % \
                (tnum, trigs0.ifo, len(t0), trigs1.ifo, len(t1)))

    i0, i1, slide = coinc.time_coincidence(t0, t1, time_window, args.timeslide_interval)
    logging.info('Coincident Trigs: %s' % (len(i1)))
    
    s0, s1 = rank_method.single(trigs0), rank_method.single(trigs1)
    c = rank_method.coinc(s0[i0], s1[i1])
    
    fi = numpy.where(slide == 0)[0]
    bi = numpy.where(slide != 0)[0]
    logging.info('%s foreground triggers' % len(fi))
    logging.info('%s background triggers' % len(bi))
    
    b = c[bi]
    sep = len(b) - args.loudest_keep
    bsort = numpy.argpartition(b, sep)
    
    bl = bi[bsort[0:sep]]
    bl = bl[slide[bl] % args.decimation_factor == 0]
    bh = bi[bsort[sep:]] 
    
    ti = numpy.concatenate([bl, bh, fi])
    
    data['stat'] += [c[ti]]
    data['decimation_factor'] += [numpy.repeat([args.decimation_factor, 1, 1], [len(bl), len(bh), len(fi)])]
    data['time1'] += [t0[i0[ti]]]
    data['time2'] += [t1[i1[ti]]]
    data['trigger_id1'] += [tid0[i0[ti]]]
    data['trigger_id2'] += [tid1[i1[ti]]]
    data['timeslide_id'] += [slide[ti]]
    data['template_id'] += [numpy.zeros(len(ti), dtype=numpy.uint32) + tnum]

logging.info('saving coincident triggers')
f = h5py.File(args.output_file, 'w')
for key in data:
    f[key] = numpy.concatenate(data[key])

f['analyzed_start'], f['analyzed_end'] = veto.segments_to_start_end(coinc_segs)
f.attrs['timeslide_interval'] = args.timeslide_interval
f.attrs['detector_1'] = det0.name
f.attrs['detector_2'] = det1.name
f.attrs['foreground_time1'] = abs(trigs0.segs)
f.attrs['foreground_time2'] = abs(trigs1.segs)
f.attrs['coinc_time'] = abs(coinc_segs)

if args.timeslide_interval:
    nslides = int(max(abs(trigs0.segs), abs(trigs1.segs)) / args.timeslide_interval)
else:
    nslides = 0
    
f.attrs['num_slides'] = nslides
logging.info('Done')
