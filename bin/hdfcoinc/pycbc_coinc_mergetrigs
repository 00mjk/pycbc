#!/usr/bin/python
""" This program adds single detector hdf trigger files together.
"""
import numpy, argparse, h5py, logging

def changes(arr):
    from pycbc.future import unique
    l = numpy.where(arr[:-1] != arr[1:])[0]
    l = numpy.concatenate(([0], l+1, [len(arr)]))
    return unique(l)

def collect(key, files):
    data = []
    for fname in files:
        fin = h5py.File(fname)
        if key in fin:
            data += [fin[key][:]]
        fin.close()
    return numpy.concatenate(data)

parser = argparse.ArgumentParser()
parser.add_argument('--trigger-files', nargs='+')
parser.add_argument('--output-file')
parser.add_argument('--bank-file')
parser.add_argument('--verbose', '-v', action='count')
args = parser.parse_args()

logging.basicConfig(format='%(asctime)s : %(message)s', level=logging.INFO) 

f = h5py.File(args.output_file, 'w')

logging.info("getting the list of columns from a representative file")
trigger_columns = []
for fname in args.trigger_files:
    f2 = h5py.File(fname, 'r')
    ifo = f2.keys()[0]
    if len(f2[ifo].keys()) > 0:
        k = f2[ifo].keys()
        trigger_columns = f2[ifo].keys()
        trigger_columns.remove('search')
        trigger_columns.remove('template_hash')
        f2.close()
        break
    f2.close()

for col in trigger_columns:
    logging.info("trigger column: %s" % col)

logging.info('reading the metadata from the files')
start = numpy.array([], dtype=numpy.float64)
end = numpy.array([], dtype=numpy.float64)
for filename in args.trigger_files:
    data = h5py.File(filename, 'r')
    s, e = data['%s/search/start_time' % ifo][:], data['%s/search/end_time' % ifo][:]
    start, end = numpy.append(start, s), numpy.append(end, e)
    data.close()    
f['%s/search/start_time' % ifo], f['%s/search/end_time' % ifo] = start, end   


logging.info('reading the trigger columns from the input files')
hashes = h5py.File(args.bank_file, 'r')['template_hash'][:]
trigger_sort = collect('%s/template_hash' % ifo, args.trigger_files).argsort()

for col in trigger_columns:
    key = '%s/%s' % (ifo, col)
    data = collect(key, args.trigger_files)
    f.create_dataset(key, data=data[trigger_sort], 
                     compression='gzip', shuffle=True, compression_opts=9) 
logging.info('done')
